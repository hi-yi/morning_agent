{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/algo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from datetime import date, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the api keys\n",
    "NEWSAPI_KEY = os.getenv('NEWSAPI_KEY')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out only credible news providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc-news</td>\n",
       "      <td>ABC News</td>\n",
       "      <td>Your trusted source for breaking news, analysi...</td>\n",
       "      <td>https://abcnews.go.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>al-jazeera-english</td>\n",
       "      <td>Al Jazeera English</td>\n",
       "      <td>News, analysis from the Middle East and worldw...</td>\n",
       "      <td>https://www.aljazeera.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ars-technica</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>The PC enthusiast's resource. Power users and ...</td>\n",
       "      <td>https://arstechnica.com</td>\n",
       "      <td>technology</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>associated-press</td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>The AP delivers in-depth coverage on the inter...</td>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>axios</td>\n",
       "      <td>Axios</td>\n",
       "      <td>Axios are a new media company delivering vital...</td>\n",
       "      <td>https://www.axios.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                name  \\\n",
       "0            abc-news            ABC News   \n",
       "1  al-jazeera-english  Al Jazeera English   \n",
       "2        ars-technica        Ars Technica   \n",
       "3    associated-press    Associated Press   \n",
       "4               axios               Axios   \n",
       "\n",
       "                                         description  \\\n",
       "0  Your trusted source for breaking news, analysi...   \n",
       "1  News, analysis from the Middle East and worldw...   \n",
       "2  The PC enthusiast's resource. Power users and ...   \n",
       "3  The AP delivers in-depth coverage on the inter...   \n",
       "4  Axios are a new media company delivering vital...   \n",
       "\n",
       "                         url    category language country  \n",
       "0     https://abcnews.go.com     general       en      us  \n",
       "1  https://www.aljazeera.com     general       en      us  \n",
       "2    https://arstechnica.com  technology       en      us  \n",
       "3        https://apnews.com/     general       en      us  \n",
       "4      https://www.axios.com     general       en      us  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract news provider list\n",
    "\n",
    "def news_source():\n",
    "    \"\"\"Fetches news source provide from News API - id, name, description, url, category, language, country\"\"\"\n",
    "    \n",
    "    source_url = \"https://newsapi.org/v2/sources\"\n",
    "    \n",
    "    params = {\n",
    "        \"apiKey\": NEWSAPI_KEY, \n",
    "    }\n",
    "\n",
    "    source_list = requests.get(source_url, params=params)\n",
    "    source_list = pd.json_normalize([news for news in source_list.json()['sources'] if (news['language'] == 'en') & \n",
    "                                    (news['country']=='us') & (news['category'] in ['general', 'technology'])])\n",
    "    return(source_list)\n",
    "\n",
    "source_list = news_source()\n",
    "# source_list.to_csv('data/source_list.csv', index=False) \n",
    "source_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_news_source(news_url):\n",
    "    \"\"\"Fetches factual reporting, traffic, and credibility score from Media Bias/Fact Check.\"\"\"\n",
    "    \n",
    "    base_search_url = \"https://mediabiasfactcheck.com/?s={}\".format(news_url)\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    try:\n",
    "        # Extract extact url for each news\n",
    "        response = requests.get(base_search_url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        result = soup.find(\"h3\", class_=\"entry-title\")\n",
    "        if not result or not result.a:\n",
    "            return {\"source\": news_url, \"factual_reporting\": \"NA\", \"traffic\": \"NA\", \"credibility\": \"NA\"}\n",
    "\n",
    "        source_page_url = result.a[\"href\"]\n",
    "\n",
    "        # Extract information for each news\n",
    "\n",
    "        response = requests.get(source_page_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        factual_reporting, traffic, credibility = \"NA\", \"NA\", \"NA\"\n",
    "\n",
    "        for para in soup.find_all(\"p\"):\n",
    "            if \"Factual Reporting:\" in para.text:\n",
    "                factual_match = re.search(r\"Factual Reporting:\\s*([^(\\n]+)\", para.text)\n",
    "                traffic_match = re.search(r\"Traffic/Popularity:\\s*([^\\n]+)\", para.text)\n",
    "                credibility_match = re.search(r\"MBFC Credibility Rating:\\s*([^\\n]+)\", para.text)\n",
    "                \n",
    "                factual_reporting = factual_match.group(1) if factual_match else \"NA\"\n",
    "                traffic = traffic_match.group(1) if traffic_match else \"NA\"\n",
    "                credibility = credibility_match.group(1) if credibility_match else \"NA\"\n",
    "                \n",
    "                break\n",
    "        \n",
    "        time.sleep(3) \n",
    "        \n",
    "        return {\n",
    "            \"source\": news_url,\n",
    "            \"factual_reporting\": factual_reporting,\n",
    "            \"traffic\": traffic,\n",
    "            \"credibility\": credibility\n",
    "        }\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return {\"source\": news_url, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>factual_reporting</th>\n",
       "      <th>traffic</th>\n",
       "      <th>credibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abc-news</td>\n",
       "      <td>ABC News</td>\n",
       "      <td>Your trusted source for breaking news, analysi...</td>\n",
       "      <td>https://abcnews.go.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>https://abcnews.go.com</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>High Traffic</td>\n",
       "      <td>HIGH CREDIBILITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ars-technica</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>The PC enthusiast's resource. Power users and ...</td>\n",
       "      <td>https://arstechnica.com</td>\n",
       "      <td>technology</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>https://arstechnica.com</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>High Traffic</td>\n",
       "      <td>HIGH CREDIBILITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>associated-press</td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>The AP delivers in-depth coverage on the inter...</td>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>High Traffic</td>\n",
       "      <td>HIGH CREDIBILITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>axios</td>\n",
       "      <td>Axios</td>\n",
       "      <td>Axios are a new media company delivering vital...</td>\n",
       "      <td>https://www.axios.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>https://www.axios.com</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>High Traffic</td>\n",
       "      <td>HIGH CREDIBILITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cbs-news</td>\n",
       "      <td>CBS News</td>\n",
       "      <td>CBS News: dedicated to providing the best in j...</td>\n",
       "      <td>http://www.cbsnews.com</td>\n",
       "      <td>general</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>http://www.cbsnews.com</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>High Traffic</td>\n",
       "      <td>HIGH CREDIBILITY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id              name  \\\n",
       "0          abc-news          ABC News   \n",
       "1      ars-technica      Ars Technica   \n",
       "2  associated-press  Associated Press   \n",
       "3             axios             Axios   \n",
       "4          cbs-news          CBS News   \n",
       "\n",
       "                                         description                      url  \\\n",
       "0  Your trusted source for breaking news, analysi...   https://abcnews.go.com   \n",
       "1  The PC enthusiast's resource. Power users and ...  https://arstechnica.com   \n",
       "2  The AP delivers in-depth coverage on the inter...      https://apnews.com/   \n",
       "3  Axios are a new media company delivering vital...    https://www.axios.com   \n",
       "4  CBS News: dedicated to providing the best in j...   http://www.cbsnews.com   \n",
       "\n",
       "     category language country                   source factual_reporting  \\\n",
       "0     general       en      us   https://abcnews.go.com             HIGH    \n",
       "1  technology       en      us  https://arstechnica.com              HIGH   \n",
       "2     general       en      us      https://apnews.com/             HIGH    \n",
       "3     general       en      us    https://www.axios.com             HIGH    \n",
       "4     general       en      us   http://www.cbsnews.com              HIGH   \n",
       "\n",
       "        traffic       credibility  \n",
       "0  High Traffic  HIGH CREDIBILITY  \n",
       "1  High Traffic  HIGH CREDIBILITY  \n",
       "2  High Traffic  HIGH CREDIBILITY  \n",
       "3  High Traffic  HIGH CREDIBILITY  \n",
       "4  High Traffic  HIGH CREDIBILITY  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out credible news sources\n",
    "news_scores = [check_news_source(u) for u in source_list['url']]\n",
    "news_scores_df = pd.DataFrame(news_scores)\n",
    "\n",
    "# Fill out elements not extracted but highly credible\n",
    "news_scores_df[news_scores_df['source']=='http://www.cbsnews.com'] = ('http://www.cbsnews.com', 'HIGH', 'High Traffic', 'HIGH CREDIBILITY') \n",
    "\n",
    "# news_scores_df.to_csv('data/news_credibility.csv', index=False)\n",
    "# Only high fact, traffic, credibility score hold news providers remained\n",
    "news_scores_df_shortlist = news_scores_df[((news_scores_df['factual_reporting'].str.contains('HIGH')) |\n",
    "                (news_scores_df['factual_reporting'].str.contains('MOSTLY'))) &\n",
    "                (news_scores_df['traffic'].str.contains('High')) &\n",
    "                (news_scores_df['credibility'].str.contains('HIGH'))]\n",
    "\n",
    "news_scores_df_shortlist = pd.merge(source_list, news_scores_df_shortlist, how='right', left_on='url', right_on='source')\n",
    "news_scores_df_shortlist.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc-news,ars-technica,associated-press,axios,cbs-news,engadget,google-news,hacker-news,national-review,nbc-news,newsweek,politico,reuters,techcrunch,techradar,the-hill,the-washington-post,time,vice-news,wired'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a list of news as a parameter\n",
    "sources_final = ','.join(news_scores_df_shortlist['id'])\n",
    "# with open('data/sources_final.txt', 'w') as f:\n",
    "#     f.write(sources_final)\n",
    "\n",
    "with open('data/sources_final.txt', 'r') as f:\n",
    "    sources_final = f.read()\n",
    "\n",
    "sources_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wired US Government Websites Are Disappearing in Real Time\n",
      "Wired Moon or Mars? The US Might Face a Tough Choice for Future Missions\n",
      "Wired Foreign Hackers Are Using Google’s Gemini in Attacks on the US\n",
      "ABC News Multiple health agency websites on HIV, contraception taken down\n",
      "ABC News Rubio to focus on curbing immigration, countering China in Latin America\n"
     ]
    }
   ],
   "source": [
    "def ai_news():\n",
    "    start_date = date.today() - timedelta(days=1)\n",
    "    end_date = date.today()\n",
    "\n",
    "    BASE_URL = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "    # Define your search query with OR\n",
    "    query = \"AI OR 'artificial intelligence' OR 'machine learning' OR 'data science' OR tech\"\n",
    "\n",
    "    # Set up request parameters\n",
    "    params = {\n",
    "        \"q\": query, \n",
    "        \"language\": \"en\",\n",
    "        \"sortBy\": \"Popularity\",\n",
    "        \"from\": str(start_date) + 'T05:00:01',\n",
    "        \"to\": str(end_date) + 'T05:00:01',\n",
    "        \"sources\": sources_final,\n",
    "        # \"searchIn\": \"title,description\"\n",
    "        \"apiKey\": NEWSAPI_KEY \n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    return(response)\n",
    "\n",
    "\n",
    "response = ai_news()\n",
    "for i in range(len(response.json()['articles']))[:5]:\n",
    "    print(response.json()['articles'][i]['source']['name'], response.json()['articles'][i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'US Government Websites Are Disappearing in Real Time',\n",
       "  'description': 'A growing number of US government websites have gone offline as of Saturday, including several related to USAID and others focused on youth programs, Africa, and more.',\n",
       "  'content': 'Several government websites have been taken down, including the usaid.gov, foreignassistance.gov, neglecteddiseases.gov, and childreninadversity.gov. A WIRED analysis of more than 1,000 federal .gov … [+3280 chars]',\n",
       "  'url': 'https://www.wired.com/story/us-government-websites-are-disappearing-in-real-time/'},\n",
       " {'title': 'Moon or Mars? The US Might Face a Tough Choice for Future Missions',\n",
       "  'description': 'Continuing the Artemis program and using its planned lunar space station as a staging post would be a more energy efficient but slower way to reach Mars, and it’s unlikely to be Elon Musk’s preference.',\n",
       "  'content': 'THIS ARTICLE IS republished fromThe Conversationunder aCreative Commons license.\\r\\nThe Artemis program has been NASAs best chance to get boots on the moon again. But with the new US administration tak… [+2495 chars]',\n",
       "  'url': 'https://www.wired.com/story/the-us-could-get-to-mars-quicker-if-it-deprioritizes-going-to-the-moon/'},\n",
       " {'title': 'Foreign Hackers Are Using Google’s Gemini in Attacks on the US',\n",
       "  'description': 'Plus: WhatsApp discloses nearly 100 targets of spyware, hackers used the AT&T breach to hunt for details on US politicians, and more.',\n",
       "  'content': 'The rapid rise of DeepSeek, a Chinese generative AI platform, heightened concerns this week over the United States AI dominance as Americans increasingly adopt Chinese-owned digital services. With on… [+3545 chars]',\n",
       "  'url': 'https://www.wired.com/story/hackers-google-gemini-us-cyberattacks/'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_input = [{key: article[key] for key in ['title','description','content','url'] if key in article} for article in response.json()['articles']]\n",
    "llm_input[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt= \"\"\"You are an AI news summarization assistant. \n",
    "Your task is to analyze a collection of news articles, identify the most frequently mentioned news topics, summarize them, and select the best representative article for each.\n",
    "\n",
    "- Introduction\n",
    "1. Group Similar News Articles: Identify common themes among the articles and group them by topic.  \n",
    "2. Filter for Tech-Related News: Only include topics related to AI, machine learning, data science, cloud computing, automation, and major tech breakthroughs. Ignore irrelevant news.  \n",
    "3. Summarize Each Topic: Provide a concise summary of each grouped topic.  \n",
    "4. Select the Best Representative URL: Choose one URL that best represents the topic (preferably from a reputable source).  \n",
    "5. Format Output as JSON.\n",
    "\n",
    "- Input Data\n",
    "{articles}\"\"\"\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"articles\"],\n",
    "    template=prompt\n",
    ")\n",
    "\n",
    "result = llm.invoke(summary_prompt.format(articles=llm_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AI Technology and DeepSeek': {'summary': 'The Chinese AI platform DeepSeek is rapidly gaining popularity, surpassing ChatGPT in growth and raising concerns about US AI dominance.  Its rapid adoption has prompted discussions about the global AI race and potential national security implications.  OpenAI has responded to the competition by releasing its o3-mini reasoning model for all ChatGPT users.',\n",
       "  'representative_url': 'https://www.techradar.com/pro/security/only-two-weeks-in-and-ai-phenomenon-deepseek-is-officially-growing-faster-than-chatgpt'},\n",
       " 'Nvidia and AI Hardware': {'summary': 'Nvidia is offering free AI courses covering generative AI, deep learning, and accelerated computing.  Performance tests of the Nvidia GeForce RTX 5090 show it significantly outperforming competitors in creative software and AI tasks.',\n",
       "  'representative_url': 'https://www.techradar.com/pro/nvidia-giving-away-free-ai-courses-worth-up-to-usd90-and-no-it-has-absolutely-nothing-to-do-with-deepseeks-ascension'},\n",
       " 'AI and Data Centers': {'summary': \"India's richest person, Mukesh Ambani, plans to build the world's largest data center, five times the capacity of Microsoft's largest, to support the growth of AI and other data-intensive technologies.  An AI engineer position at Reprompt focuses on building world-class location data using LLMs.\",\n",
       "  'representative_url': 'https://www.techradar.com/pro/indias-richest-person-wants-to-build-the-worlds-largest-data-center-five-times-the-capacity-of-microsofts-mega-site'},\n",
       " 'AI Agents and Unicorns': {'summary': 'The potential for AI agents to create one-person unicorn companies is being discussed, along with the societal implications of such a development.',\n",
       "  'representative_url': 'https://techcrunch.com/2025/02/01/ai-agents-could-birth-the-first-one-person-unicorn-but-at-what-societal-cost/'},\n",
       " 'Generative AI and GANs': {'summary': 'Explanations of Generative Adversarial Networks (GANs) and their role in creating synthetic data are gaining attention.',\n",
       "  'representative_url': 'https://www.techradar.com/computing/artificial-intelligence/what-is-a-generative-adversarial-network'},\n",
       " 'AI-Powered Content Creation Tools': {'summary': 'New AI-powered tools are emerging for video editing and social media content creation, offering automated solutions for content creators.',\n",
       "  'representative_url': 'https://news.ycombinator.com/item?id=42903519'}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_json = result.to_json()['kwargs']['content'].strip(\"'```json\\n\")\n",
    "news_summary = json.loads(result_json)['news_summary']\n",
    "news_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'AI Technology and DeepSeek',\n",
       "  'summary': 'The Chinese AI platform DeepSeek is rapidly gaining popularity, surpassing ChatGPT in growth and raising concerns about US AI dominance.  Its rapid adoption has prompted discussions about the global AI race and potential national security implications.  OpenAI has responded to the competition by releasing its o3-mini reasoning model for all ChatGPT users.',\n",
       "  'representative_url': 'https://www.techradar.com/pro/security/only-two-weeks-in-and-ai-phenomenon-deepseek-is-officially-growing-faster-than-chatgpt'},\n",
       " {'title': 'Nvidia and AI Hardware',\n",
       "  'summary': 'Nvidia is offering free AI courses covering generative AI, deep learning, and accelerated computing.  Performance tests of the Nvidia GeForce RTX 5090 show it significantly outperforming competitors in creative software and AI tasks.',\n",
       "  'representative_url': 'https://www.techradar.com/pro/nvidia-giving-away-free-ai-courses-worth-up-to-usd90-and-no-it-has-absolutely-nothing-to-do-with-deepseeks-ascension'},\n",
       " {'title': 'AI and Data Centers',\n",
       "  'summary': \"India's richest person, Mukesh Ambani, plans to build the world's largest data center, five times the capacity of Microsoft's largest, to support the growth of AI and other data-intensive technologies.  An AI engineer position at Reprompt focuses on building world-class location data using LLMs.\",\n",
       "  'representative_url': 'https://www.techradar.com/pro/indias-richest-person-wants-to-build-the-worlds-largest-data-center-five-times-the-capacity-of-microsofts-mega-site'},\n",
       " {'title': 'AI Agents and Unicorns',\n",
       "  'summary': 'The potential for AI agents to create one-person unicorn companies is being discussed, along with the societal implications of such a development.',\n",
       "  'representative_url': 'https://techcrunch.com/2025/02/01/ai-agents-could-birth-the-first-one-person-unicorn-but-at-what-societal-cost/'},\n",
       " {'title': 'Generative AI and GANs',\n",
       "  'summary': 'Explanations of Generative Adversarial Networks (GANs) and their role in creating synthetic data are gaining attention.',\n",
       "  'representative_url': 'https://www.techradar.com/computing/artificial-intelligence/what-is-a-generative-adversarial-network'},\n",
       " {'title': 'AI-Powered Content Creation Tools',\n",
       "  'summary': 'New AI-powered tools are emerging for video editing and social media content creation, offering automated solutions for content creators.',\n",
       "  'representative_url': 'https://news.ycombinator.com/item?id=42903519'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_summary_ls = []\n",
    "for k, v in news_summary.items():\n",
    "    l = {}\n",
    "    l['title'] = k\n",
    "    for k2, v2 in v.items():\n",
    "       l[k2] = v2\n",
    "    news_summary_ls.append(l)\n",
    "# news_summary_ls = pd.DataFrame(news_summary_ls)\n",
    "news_summary_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
